{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Library"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-02-19T15:00:52.114153Z",
                    "iopub.status.busy": "2023-02-19T15:00:52.113743Z",
                    "iopub.status.idle": "2023-02-19T15:00:52.122514Z",
                    "shell.execute_reply": "2023-02-19T15:00:52.121483Z",
                    "shell.execute_reply.started": "2023-02-19T15:00:52.11412Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import ast\n",
                "import json\n",
                "import os\n",
                "from glob import glob\n",
                "from typing import List\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "\n",
                "tqdm.pandas()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Donwload input datasets from https://fielddaylab.wisc.edu/opengamedata/\n",
                "INPUT_DIR = \"/home/jovyan/work/input/raw/event_log\"\n",
                "OUTPUT_DIR = \"/home/jovyan/work/input/preprocess/from_event_log_sample\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Processing Function"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Constant"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EVENT_DATA_COLUMNS = [\n",
                "    'event_custom',\n",
                "    'fullscreen',\n",
                "    'cur_cmd_fqid',\n",
                "    'type',\n",
                "    'save_code',\n",
                "    'question',\n",
                "    'response_index',\n",
                "    'music',\n",
                "    'question_index',\n",
                "    'server_time',\n",
                "    'hq',\n",
                "    'response',\n",
                "    'fqid',\n",
                "    'subtype',\n",
                "    'script_version',\n",
                "    'script_type',\n",
                "    'page',\n",
                "    'cur_cmd_type',\n",
                "    'remote_addr',\n",
                "    'persistent_session_id',\n",
                "    'http_user_agent',\n",
                "    'screen_coor',\n",
                "    'room_fqid',\n",
                "    'name',\n",
                "    'room_coor',\n",
                "    'end_time',\n",
                "    'interacted_fqid',\n",
                "    'start_time',\n",
                "    'text_fqid',\n",
                "    'level',\n",
                "    'text',\n",
                "    'quiz_number'\n",
                "]\n",
                "\n",
                "NG_FQIDS = [\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_0_fail'],\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_1_fail'],\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_plaquefirst_0_fail',  'tunic.capitol_0.hall.boss.chap1_finale_slipfirst_0_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_0_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_1_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_2_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_3_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_4_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_5_fail'],\n",
                "    ['tunic.capitol_1.hall.gramps.chap2_teddy_finale_0_fail'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_teddy_finale_1_fail'],\n",
                "    ['tunic.capitol_1.hall.gramps.chap2_teddy_finale_2_fail'],\n",
                "    ['tunic.capitol_1.hall.wells.chap2_teddy_finale_3_fail'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_0_fail'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_1_fail'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_2_fail'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_3_fail'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_4_fail']\n",
                "]\n",
                "\n",
                "OK_FQIDS = [\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_1'],\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_plaquefirst_0', 'tunic.capitol_0.hall.boss.chap1_finale_slipfirst_0'],\n",
                "    ['tunic.capitol_0.hall.boss.chap1_finale_plaquefirst_1', 'tunic.capitol_0.hall.boss.chap1_finale_slipfirst_1'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_1'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_2'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_3'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_4'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_5'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_finale_6'],\n",
                "    ['tunic.capitol_1.hall.boss.chap2_teddy_finale_1'],\n",
                "    ['tunic.capitol_1.hall.gramps.chap2_teddy_finale_2'],\n",
                "    ['tunic.capitol_1.hall.wells.chap2_teddy_finale_3'],\n",
                "    ['tunic.capitol_1.hall.gramps.chap2_teddy_finale_4'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_1'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_2'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_3'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_4'],\n",
                "    ['tunic.capitol_2.hall.boss.chap4_finale_5']\n",
                "]\n",
                "\n",
                "FEATURE_COLUMNS = ['session_id', 'index', 'elapsed_time', 'event_name', 'name',\n",
                "'level', 'page', 'room_coor_x', 'room_coor_y', 'screen_coor_x',\n",
                "'screen_coor_y', 'hover_duration', 'text', 'fqid', 'room_fqid',\n",
                "'text_fqid', 'fullscreen', 'hq', 'music', 'level_group']\n",
                "\n",
                "REPLACE_WORDS_PAIRS =[\n",
                "    ('ðŸ˜´', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4'),\n",
                "    ('I used to have a magnifying glass around hereâ€¦',\n",
                "    'I used to have a magnifying glass around here\\\\u00e2\\\\u20ac\\\\u00a6'),\n",
                "    ('\"Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\"',\n",
                "    '\\\\Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\\\\'),\n",
                "    ('Great Scott, youâ€™re right!', \"Great Scott, you're right!\"),\n",
                "    ('Iâ€™ve got Wellsâ€™s ID!', \"I've got Wells's ID!\"),\n",
                "    ('Itâ€™s locked!', \"It's locked!\"),\n",
                "    ('ðŸ˜\\xad', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00ad'),\n",
                "    ('Hang on. Iâ€™ll get you out of there!',\n",
                "    \"Hang on. I'll get you out of there!\"),\n",
                "    ('â\\x9d¤ï¸\\x8f', '\\\\u00e2\\\\u009d\\\\u00a4\\\\u00ef\\\\u00b8\\\\u008f'),\n",
                "    ('ðŸ˜\\x90', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0090'),\n",
                "    ('ðŸ˜\\x9d', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u009d'),\n",
                "    ('ðŸ§˜', '\\\\u00f0\\\\u0178\\\\u00a7\\\\u02dc'),\n",
                "    ('ðŸ\\x8d©', '\\\\u00f0\\\\u0178\\\\u008d\\\\u00a9'),\n",
                "    ('ðŸ¦—', '\\\\u00f0\\\\u0178\\\\u00a6\\\\u2014'),\n",
                "    ('ðŸ˜Š', '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0160'),\n",
                "    ('Ooh... \"Ecology flag, by Ron Cobb.\"',\n",
                "    'Ooh... \\\\Ecology flag, by Ron Cobb.\\\\'),\n",
                "    ('ðŸ™„', '\\\\u00f0\\\\u0178\\\\u2122\\\\u201e'),\n",
                "    ('He\\'s wrong about old shirts and his name rhymes with \"smells\"...',\n",
                "    \"He's wrong about old shirts and his name rhymes with \\\\smells\\\\...\"),\n",
                "    (\"Heâ€™s always trying to get you in trouble, and he doesn't like animals!\",\n",
                "    \"He's always trying to get you in trouble, and he doesn't like animals!\"),\n",
                "    ('\"Ecology flag, by Ron Cobb.\"', '\\\\Ecology flag, by Ron Cobb.\\\\')\n",
                " ]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def decode_raw_log_data(df_log: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Load raw event data and convert to structured data\n",
                "\n",
                "    Args:\n",
                "        df_log (pd.DataFrame): Raw log data.\n",
                "\n",
                "    Returns:\n",
                "        pd.DataFrame: Structured log data.\n",
                "    \"\"\"\n",
                "    df_log = df_log.rename(columns={\"client_time\": \"timestamp\", \"event_data_complex\": \"event_data\"})\n",
                "    \n",
                "    # Convert event_data column values from str to dict.\n",
                "    # Whether ast or json should be used depends on the data.\n",
                "    use_ast = (df_log[\"event_data\"].str.contains(\"\\\"fqid\\\"\")).sum() / len(df_log) < 0.7\n",
                "    if use_ast:\n",
                "        df_log[\"event_data\"] = [ast.literal_eval(d) for d in df_log[\"event_data\"]]\n",
                "    else:\n",
                "        df_log[\"event_data\"] = [json.loads(d) for d in df_log[\"event_data\"]]\n",
                "    \n",
                "    # Extract log data pushed into one column into different columns for each item.\n",
                "    for col in EVENT_DATA_COLUMNS:\n",
                "        df_log[col] = df_log[\"event_data\"].apply(lambda row: row.get(col, np.nan))\n",
                "    return df_log\n",
                "\n",
                "\n",
                "def create_label_dataframe(df_log: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Create label datframe.\n",
                "\n",
                "    Args:\n",
                "        df_log (pd.DataFrame): Structured log data.\n",
                "\n",
                "    Returns:\n",
                "        pd.DataFrame: Label dataframe.\n",
                "    \"\"\"\n",
                "    df_correct = df_log[[\"session_id\"]].drop_duplicates().set_index(\"session_id\")\n",
                "    df_uncorrect = df_correct.copy()\n",
                "\n",
                "    # Judges correct and incorrect answers for each session_id x question.\n",
                "    def count_correct(group, ok_fqids_target_q: List[str], ng_fqids_target_q: List[str]):\n",
                "        return pd.Series([int((group[\"cur_cmd_fqid\"].isin(ok_fqids_target_q)).sum() > 0), int((group[\"cur_cmd_fqid\"].isin(ng_fqids_target_q)).sum() > 0)])\n",
                "\n",
                "    for q in range(18):\n",
                "        res = df_log.groupby(\"session_id\").apply(lambda x: count_correct(x, OK_FQIDS[q], NG_FQIDS[q]))\n",
                "        res = res.rename(columns={0: f\"correct_{q+1}\", 1: f\"uncorrect_{q+1}\"})\n",
                "        df_correct = df_correct.join(res[f\"correct_{q+1}\"], how=\"left\")\n",
                "        df_uncorrect = df_uncorrect.join(res[f\"uncorrect_{q+1}\"], how=\"left\")\n",
                "    \n",
                "    # Put np.nan, not 0 or 1, in the label for players who retired in the middle of the game.\n",
                "    # correct: correct_{q} == 1 and uncorrect_{q} == 0\n",
                "    # uncorrect: correct_{q} == 1 and uncorrect_{q} == 1\n",
                "    # retire: correct_{q} == 0\n",
                "    corrects = df_correct.values - df_uncorrect.values\n",
                "    corrects = np.where(df_correct.values == 0, np.nan, corrects)\n",
                "\n",
                "    # Convert to vertical holding.\n",
                "    df_correct[df_correct.columns] = corrects\n",
                "    df_correct = df_correct.reset_index()\n",
                "    df_correct_vertical = pd.DataFrame(columns=[\"session_id\", \"correct\"])\n",
                "\n",
                "    for q in range(1, 19):\n",
                "        df_correct_target_q = df_correct[[\"session_id\", f\"correct_{q}\"]].rename(columns={f\"correct_{q}\": \"correct\"}).copy()\n",
                "        df_correct_target_q[\"session_id\"] = df_correct_target_q[\"session_id\"].astype(str) + \"_q\" + str(q)\n",
                "        df_correct_vertical = pd.concat([df_correct_vertical, df_correct_target_q], axis=0)\n",
                "    df_correct_vertical = df_correct_vertical.sort_values(\"session_id\")\n",
                "\n",
                "    return df_correct_vertical\n",
                "\n",
                "\n",
                "def clean_feature_columns(df_log: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Perform a simple cleansing of the dataframe.\n",
                "\n",
                "    Args:\n",
                "        df_log (pd.DataFrame): Structured log data.\n",
                "\n",
                "    Returns:\n",
                "        pd.DataFrame: Label dataframe.\n",
                "    \"\"\"\n",
                "    # Preprocess coor columns.\n",
                "    df_log[[\"room_coor_x\", \"room_coor_y\"]] = df_log[\"room_coor\"].progress_apply(pd.Series).set_axis(['room_coor_x', 'room_coor_y'], axis=1)\n",
                "    df_log[[\"screen_coor_x\", \"screen_coor_y\"]] = df_log[\"screen_coor\"].progress_apply(pd.Series).set_axis(['screen_coor_x', 'screen_coor_y'], axis=1)\n",
                "\n",
                "    # Typing.\n",
                "    df_log[\"level\"] = df_log[\"level\"].astype(int)\n",
                "    df_log[\"index\"] = df_log[\"index\"].astype(int)\n",
                "    df_log[\"elapsed_time\"] = df_log[\"elapsed_time\"].astype(int)\n",
                "    \n",
                "    # Create level_group.\n",
                "    df_log[\"level_group\"] = np.nan\n",
                "    df_log.loc[df_log[\"level\"]<=4, \"level_group\"] = \"0-4\"\n",
                "    df_log.loc[(df_log[\"level\"]>4)&(df_log[\"level\"]<=12), \"level_group\"] = \"5-12\"\n",
                "    df_log.loc[(df_log[\"level\"]>12)&(df_log[\"level\"]<=22), \"level_group\"] = \"13-22\"\n",
                "\n",
                "    # Other preprocess.\n",
                "    df_log[\"hover_duration\"] = df_log[\"end_time\"] - df_log[\"start_time\"]\n",
                "    df_log[\"event_name\"] = df_log[\"subtype\"] + \"_\" + df_log[\"type\"]\n",
                "    df_log.loc[df_log[\"type\"]==\"checkpoint\", \"event_name\"] = df_log[\"type\"]\n",
                "    df_log.loc[df_log[\"text\"]=='null', \"text\"] = np.nan\n",
                "\n",
                "    return df_log\n",
                "\n",
                "\n",
                "def create_feature_dataframe(df_log: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"Create feature datframe.\n",
                "\n",
                "    Args:\n",
                "        df_log (pd.DataFrame): Structured log data.\n",
                "\n",
                "    Returns:\n",
                "        pd.DataFrame: Label dataframe.\n",
                "    \"\"\"\n",
                "    df_log[\"timestamp\"] = pd.to_datetime(df_log[\"timestamp\"])\n",
                "\n",
                "    def groupby_func(df_target):\n",
                "        # Drop some records.\n",
                "        df_target = df_target[~df_target[\"type\"].isin([\"quizquestion\", \"endgame\", \"quiz\"])]\n",
                "        df_target = df_target[~df_target[\"event_custom\"].isin([1, 11, 12, 23, 24])]\n",
                "        df_target = df_target[df_target[\"name\"]!=\"choice\"]\n",
                "        df_target = df_target[df_target[\"subtype\"]!=\"wildcard\"]\n",
                "        df_target = df_target[df_target[\"room_fqid\"]!=\"tunic.nelson.trail\"]\n",
                "        df_target = df_target[~df_target[\"level\"].isin([23, 24])]\n",
                "\n",
                "        df_target = df_target[~((df_target[\"subtype\"]==\"notebook\")&(df_target[\"room_fqid\"]==\"tunic.capitol_0.hall\"))]\n",
                "        df_target = df_target[~((df_target[\"subtype\"]==\"notebook\")&(df_target[\"room_fqid\"]==\"tunic.capitol_1.hall\"))]\n",
                "        df_target = df_target[~((df_target[\"subtype\"]==\"notebook\")&(df_target[\"room_fqid\"]==\"tunic.capitol_2.hall\"))]\n",
                "\n",
                "        # Reindex.\n",
                "        df_target[\"index\"] = range(len(df_target))\n",
                "\n",
                "        # Drop chap2_finale except checkpoint.\n",
                "        df_target = df_target[(df_target[\"type\"]==\"checkpoint\")|(df_target[\"fqid\"]!=\"chap2_finale\")]\n",
                "\n",
                "        # Calculate elapsed_time.\n",
                "        df_target[\"elapsed_time\"] = (df_target[\"timestamp\"]-df_target[\"timestamp\"].min()).map(lambda x: 1000*x.total_seconds())\n",
                "        df_target[\"elapsed_time\"] = df_target[\"elapsed_time\"].astype(int)\n",
                "\n",
                "        # In each level group, drop after the checkpoint.\n",
                "        # chap1\n",
                "        df_1 = df_target[(df_target[\"type\"]==\"checkpoint\")&(df_target[\"fqid\"]==\"chap1_finale_c\")]\n",
                "        if len(df_1)>0:\n",
                "            end_time_1 = df_1[\"elapsed_time\"].values[0]\n",
                "            df_target = df_target[~((df_target[\"text_fqid\"]==\"tunic.capitol_0.hall.chap1_finale_c\")&(df_target[\"elapsed_time\"]>end_time_1))]\n",
                "\n",
                "        # chap2\n",
                "        df_2 = df_target[(df_target[\"type\"]==\"checkpoint\")&(df_target[\"fqid\"]==\"chap2_finale_c\")]\n",
                "        if len(df_2)>0:\n",
                "            end_time_2 = df_2[\"elapsed_time\"].values[0]\n",
                "            df_target = df_target[~((df_target[\"text_fqid\"]==\"tunic.capitol_1.hall.chap2_finale_c\")&(df_target[\"elapsed_time\"]>end_time_2))]\n",
                "\n",
                "        # chap4\n",
                "        df_4 = df_target[(df_target[\"type\"]==\"checkpoint\")&(df_target[\"fqid\"]==\"chap4_finale_c\")]\n",
                "        if len(df_4)>0:\n",
                "            end_time_4 = df_4[\"elapsed_time\"].values[0]\n",
                "            df_target = df_target[~((df_target[\"text_fqid\"]==\"tunic.capitol_2.hall.chap4_finale_c\")&(df_target[\"elapsed_time\"]>end_time_4))]\n",
                "\n",
                "        # Take the offset of index.\n",
                "        df_start = df_target[df_target[\"text_fqid\"]==\"tunic.historicalsociety.closet.intro\"][\"index\"]\n",
                "        if len(df_start) > 0:\n",
                "            start_index = df_start.values[0]\n",
                "            df_target[\"index\"] = (df_target[\"index\"]-start_index)\n",
                "        else:\n",
                "            df_target = pd.DataFrame()\n",
                "        return df_target\n",
                "\n",
                "    # Drop and cleanse unnecessary records.\n",
                "    df_all = df_log.groupby(\"session_id\").progress_apply(groupby_func)\n",
                "    df_all = df_all.drop(columns=[\"session_id\"]).reset_index().drop(columns=\"level_1\")\n",
                "    df_all = clean_feature_columns(df_all)\n",
                "    df_all = df_all[FEATURE_COLUMNS]\n",
                "\n",
                "    # Replace some texts.\n",
                "    for i in range(len(REPLACE_WORDS_PAIRS)):\n",
                "        before_word = REPLACE_WORDS_PAIRS[i][0]\n",
                "        after_word = REPLACE_WORDS_PAIRS[i][1]\n",
                "        df_all.loc[df_all[\"text\"]==before_word, \"text\"] = after_word\n",
                "    df_all.loc[df_all[\"fqid\"]==\"0\", \"fqid\"] = np.nan\n",
                "    return df_all\n",
                "\n",
                "\n",
                "def create_feature_and_label_csv(input_file: str) -> None:\n",
                "    \"\"\"Create feature csv and label csv files from raw log data in tsv files.\n",
                "\n",
                "    Args:\n",
                "        input_file (str): tsv file path.\n",
                "    \"\"\"\n",
                "    feature_fn = OUTPUT_DIR + \"/\" + input_file.split(\"/\")[-2] + \"_feature.csv\"\n",
                "    label_fn = OUTPUT_DIR + \"/\" + input_file.split(\"/\")[-2] + \"_label.csv\"\n",
                "\n",
                "    if os.path.isfile(feature_fn):\n",
                "        print(\"Skip existed datasets:\", input_file)\n",
                "        return\n",
                "\n",
                "    df_log = pd.read_table(input_file)\n",
                "\n",
                "    if len(df_log) < 5:\n",
                "        print(\"Skip few logs:\", input_file)\n",
                "        return\n",
                "\n",
                "    df_log = decode_raw_log_data(df_log)\n",
                "    df_label = create_label_dataframe(df_log)\n",
                "    df_feature = create_feature_dataframe(df_log)\n",
                "\n",
                "    df_label.to_csv(label_fn, index=False)\n",
                "    df_feature.to_csv(feature_fn, index=False)\n",
                "\n",
                "    print(\"Processed:\", input_file)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Processing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate datasets from raw log data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "all_tsv = glob(f\"{INPUT_DIR}/*/*/*.tsv\")\n",
                "\n",
                "# Sort by file size.\n",
                "file_sizes = {}\n",
                "for file_path in all_tsv:\n",
                "    file_sizes[file_path] = os.path.getsize(file_path)\n",
                "all_tsv = sorted(all_tsv, key=lambda x: file_sizes[x])\n",
                "\n",
                "# Without multiprocessing ver because of memory limit.\n",
                "for input_file in all_tsv:\n",
                "    create_feature_and_label_csv(input_file)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Concat all datasets into one"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = glob(f\"{OUTPUT_DIR}/*_feature.csv\")\n",
                "labels = glob(f\"{OUTPUT_DIR}/*_label.csv\")\n",
                "\n",
                "df_features = pd.DataFrame()\n",
                "for feature in tqdm(features):\n",
                "    df_feature = pd.read_csv(feature)\n",
                "    df_features = pd.concat([df_features, df_feature], axis=0)\n",
                "\n",
                "df_labels = pd.DataFrame()\n",
                "for label in tqdm(labels):\n",
                "    df_label = pd.read_csv(label)\n",
                "    df_labels = pd.concat([df_labels, df_label], axis=0)\n",
                "\n",
                "df_features.to_csv(f\"{OUTPUT_DIR}/all_features.csv\", index=False)\n",
                "df_labels.to_csv(f\"{OUTPUT_DIR}/all_labels.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.12"
        },
        "vscode": {
            "interpreter": {
                "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
